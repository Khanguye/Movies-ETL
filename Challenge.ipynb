{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constant Data Path\n",
    "DATA_PATH = \"data/\"\n",
    "#File names: assume that the updated data will stay in the same formats\n",
    "wikipedia_filename = \"wikipedia.movies.json\"\n",
    "kaggle_meta_filname = \"movies_metadata.csv\"\n",
    "kaggle_rating_filename = \"ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ETL Process function with 3 data sources\n",
    "def ETL_Movies_Process(wikipedia_filename,kaggle_meta_filname,kaggle_rating_filename):\n",
    "    #####################################################################\n",
    "    #Select only movies and not TV Show\n",
    "    def filter_movies(wiki_movies_raw_list):\n",
    "        # Find all records have Director or Directed and imdb_link and not TV Show by attributes\n",
    "        ### make sure movie is a dictionary type ###\n",
    "        wiki_movies = [movie for movie in wiki_movies_raw_list\\\n",
    "                   if  type(movie) == dict\n",
    "                       and ('Director' in movie or 'Directed by' in movie)\\\n",
    "                       and 'imdb_link' in movie\\\n",
    "                       and 'No. of episodes' not in movie]\n",
    "        return wiki_movies\n",
    "    ######################################################################\n",
    "    #Normalize each movie the same attributes\n",
    "    def clean_movie(movie):\n",
    "        #any error during normalize movie then return None\n",
    "        try:\n",
    "            movie = dict(movie) #create a non-destructive copy\n",
    "            alt_titles = {}\n",
    "            #insert titles attribute in a dictionary of alter_titles\n",
    "            #delete the attribute from movie dictionary\n",
    "            for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                        'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                        'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                        'Revised Romanization','Romanized','Russian',\n",
    "                        'Simplified','Traditional','Yiddish']:\n",
    "                 if key in movie:\n",
    "                    alt_titles[key] = movie[key]\n",
    "                    movie.pop(key)\n",
    "\n",
    "            if len(alt_titles) > 0:\n",
    "                movie['alt_titles'] = alt_titles\n",
    "            # merge column names\n",
    "            def change_column_name(old_name, new_name):\n",
    "                if old_name in movie:\n",
    "                    movie[new_name] = movie.pop(old_name)\n",
    "            change_column_name('Adaptation by', 'Writer(s)')\n",
    "            change_column_name('Country of origin', 'Country')\n",
    "            change_column_name('Directed by', 'Director')\n",
    "            change_column_name('Distributed by', 'Distributor')\n",
    "            change_column_name('Edited by', 'Editor(s)')\n",
    "            change_column_name('Length', 'Running time')\n",
    "            change_column_name('Original release', 'Release date')\n",
    "            change_column_name('Music by', 'Composer(s)')\n",
    "            change_column_name('Produced by', 'Producer(s)')\n",
    "            change_column_name('Producer', 'Producer(s)')\n",
    "            change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "            change_column_name('Productioncompany ', 'Production company(s)')\n",
    "            change_column_name('Released', 'Release Date')\n",
    "            change_column_name('Release Date', 'Release date')\n",
    "            change_column_name('Screen story by', 'Writer(s)')\n",
    "            change_column_name('Screenplay by', 'Writer(s)')\n",
    "            change_column_name('Story by', 'Writer(s)')\n",
    "            change_column_name('Theme music composer', 'Composer(s)')\n",
    "            change_column_name('Written by', 'Writer(s)')\n",
    "            return movie\n",
    "        except:\n",
    "            print(\"<ERROR>\",\"clean_movie\",movie)\n",
    "            return None\n",
    "    #######################################################################\n",
    "    def movies_dataframe(wiki_movies_filename):\n",
    "        #Read wikipedia movies json file\n",
    "        with open(f\"{DATA_PATH}{wiki_movies_filename}\", mode='r') as file:\n",
    "            wiki_movies_raw_list = json.load(file)\n",
    "        #select only movies\n",
    "        wiki_movies = filter_movies(wiki_movies_raw_list)\n",
    "        #clean up movie attributes\n",
    "        #clean_movie can return None if movie item causes any issue during normalize\n",
    "        clean_wiki_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "        #make sure remove None\n",
    "        clean_wiki_movies = [movie for movie in clean_wiki_movies if movie is not None]\n",
    "        #creat dataframe for movies\n",
    "        wiki_movies_df = pd.DataFrame(clean_wiki_movies)\n",
    "    \n",
    "        return wiki_movies_df\n",
    "    #######################################################################\n",
    "    def parse_dollars(s):\n",
    "        try:\n",
    "            # if s is not a string, return NaN\n",
    "            if type(s) != str:\n",
    "                return np.nan\n",
    "            # if input is of the form $###.# million\n",
    "            if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "                # remove dollar sign and \" million\"\n",
    "                s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "                # convert to float and multiply by a million\n",
    "                value = float(s) * 10**6\n",
    "                # return value\n",
    "                return value\n",
    "            # if input is of the form $###.# billion\n",
    "            elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "                # remove dollar sign and \" billion\"\n",
    "                s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "                # convert to float and multiply by a billion\n",
    "                value = float(s) * 10**9\n",
    "                # return value\n",
    "                return value\n",
    "            # if input is of the form $###,###,###\n",
    "            elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "                # remove dollar sign and commas\n",
    "                s = re.sub('\\$|,','', s)\n",
    "                # convert to float\n",
    "                value = float(s)\n",
    "                # return value\n",
    "                return value\n",
    "            # otherwise, return NaN\n",
    "            else:\n",
    "                return np.nan\n",
    "        except:\n",
    "            print(\"<Error>\",\"parse_dollars\", s)\n",
    "            return np.nan\n",
    "    #######################################################################\n",
    "    def parse_to_number(dataframe, oldcolumn_name, newcolumn_name):\n",
    "        try:\n",
    "            #range pattern: $38.9–40.3 million \n",
    "            range_pattern = r'\\$.*[-—–](?![a-z])'\n",
    "            #Pattern $20.0 million or $ 1.02 billion or $2,030 million or $1.234.334 million\n",
    "            form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "            form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "\n",
    "            #tranform the column values to uniform value for future analytic query\n",
    "            #get series column and drop all none value  \n",
    "            column_series = dataframe[oldcolumn_name].dropna() \n",
    "            #flat out the columns value list to string\n",
    "            column_series = column_series.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "            #replace range pattern with $\n",
    "            column_series = column_series.str.replace(range_pattern, '$', regex=True)\n",
    "            #replace [9] to empty string\n",
    "            column_series = column_series.str.replace(r'\\[\\d+\\]\\s*', '',regex=True)\n",
    "            #add new column \n",
    "            dataframe[newcolumn_name] = column_series.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "            #remove the old column\n",
    "            dataframe.drop(oldcolumn_name, axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"<Error>\",\"parse_to_number\",oldcolumn_name,newcolumn_name)\n",
    "    #######################################################################\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "    #######################################################################\n",
    "    def tranform_wiki_movies(wikipedia_filename):\n",
    "        #load a clean movie dataframe\n",
    "        wiki_movies_df = movies_dataframe(wikipedia_filename)\n",
    "        \n",
    "        #remove all duplicate rows that have the same imdb_id\n",
    "        #imdb_id can extract from imdb_link (Patter: tt1234567)\n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "        wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "        \n",
    "        #get columns that contains less than 90% of all values\n",
    "        #Project to new set of columns\n",
    "        wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9] \n",
    "        wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "        \n",
    "        #Create a new column to number value from old column string\n",
    "        #box_office\n",
    "        parse_to_number(wiki_movies_df,'Box office', 'box_office')\n",
    "        #budget\n",
    "        parse_to_number(wiki_movies_df,'Budget', 'budget')\n",
    "        \n",
    "        #date pattern\n",
    "        # May 17, 1990\n",
    "        date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "        # 1990.03.24\n",
    "        date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "        # July 1998\n",
    "        date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "        # 2018\n",
    "        date_form_four = r'\\d{4}'\n",
    "        \n",
    "        #transfrom Release date to uniform datetime\n",
    "        release_date = wiki_movies_df['Release date'].dropna()\n",
    "        release_date = release_date.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "        release_date = release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')\n",
    "        wiki_movies_df['release_date'] = pd.to_datetime(release_date[0], infer_datetime_format=True)\n",
    "        wiki_movies_df.drop('Release date', axis=1, inplace=True)\n",
    "        \n",
    "        #tranform running time to number of minutes\n",
    "        running_time = wiki_movies_df['Running time'].dropna()\n",
    "        running_time = running_time.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "        running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "        running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "        wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "        wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "        \n",
    "        return wiki_movies_df\n",
    "    #######################################################################\n",
    "    def tranform_kaggle_metadata(kaggle_meta_filname):\n",
    "        #load kaggle_metadata\n",
    "        kaggle_metadata_df = pd.read_csv(f'{DATA_PATH}{kaggle_meta_filname}',low_memory=False)\n",
    "        #keep the row with adult is false and drop adult column after\n",
    "        kaggle_metadata_df = kaggle_metadata_df.loc[kaggle_metadata_df['adult'] == 'False'].drop('adult',axis='columns')\n",
    "\n",
    "        #Turn off all error to NaN ot NaT instead of raise error by default.\n",
    "        #If ‘coerce’, then invalid parsing will be set as NaN. instead raise issue\n",
    "\n",
    "        #convert to boolean for video column\n",
    "        kaggle_metadata_df['video'] = kaggle_metadata_df['video'] == 'True'\n",
    "        #convert to number for budget column\n",
    "        kaggle_metadata_df['budget'] = pd.to_numeric(kaggle_metadata_df['budget'],errors='coerce') \n",
    "        #convert to number for id column\n",
    "        kaggle_metadata_df['id'] = pd.to_numeric(kaggle_metadata_df['id'], errors='coerce')\n",
    "        #convert to number for popularity column\n",
    "        kaggle_metadata_df['popularity'] = pd.to_numeric(kaggle_metadata_df['popularity'], errors='coerce')\n",
    "        #convert to datetime for release_date\n",
    "        kaggle_metadata_df['release_date'] = pd.to_datetime(kaggle_metadata_df['release_date'], errors='coerce')\n",
    " \n",
    "        return kaggle_metadata_df\n",
    "    #######################################################################\n",
    "\n",
    "    #######################################################################\n",
    "    \n",
    "    #Extract and Transform wikipedia movie to meaningful data\n",
    "    wiki_movies_df = tranform_wiki_movies(wikipedia_filename)\n",
    "    #Extract and Transform kaggle meta to meaningful data\n",
    "    kaggle_metadata_df = tranform_kaggle_metadata(kaggle_meta_filname)\n",
    "   \n",
    "    ######################################################################\n",
    "    \n",
    "    #Merge data on imdb_id with inner join\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata_df, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "    #Drop all movies release date\n",
    "    movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n",
    "    #Let Drop douplicated columns and keep more accurate column information\n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "    #Let Fill the missing data to one column before drop the orther\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "    \n",
    "    #project dataframe to new set columns\n",
    "    movies_df = movies_df[['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]\n",
    "    \n",
    "    #Rename column names\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)\n",
    "    \n",
    "    #####################################################\n",
    "    \n",
    "    #Save to Database\n",
    "    db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "    engine = create_engine(db_string)\n",
    "    \n",
    "    #Drop table before inserts\n",
    "    movies_df.to_sql(name='movies', con=engine,if_exists='replace')\n",
    "    print(\"Total rows export to SQL:\",len(movies_df))\n",
    "    print(\"Export Movies dataframe was successfull\")\n",
    "    \n",
    "    \n",
    "    # create a variable for the number of rows imported\n",
    "    rows_imported = 0\n",
    "    # get the start_time from time.time()\n",
    "    start_time = time.time()\n",
    "    # first read\n",
    "    is_first_read = True\n",
    "    for data in pd.read_csv(f'{DATA_PATH}{kaggle_rating_filename}', chunksize=1000000):\n",
    "\n",
    "        # print out the range of rows that are being imported\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "       \n",
    "        #End sure drop table before insert\n",
    "        if is_first_read:\n",
    "            data.to_sql(name='ratings', con=engine, if_exists='replace')\n",
    "            is_first_read=False\n",
    "        else:\n",
    "            data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "        # increment the number of rows imported by the chunksize\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # add elapsed time to final print out\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')\n",
    "    \n",
    "    print(\"ETL was completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows export to SQL: 6051\n",
      "Export Movies dataframe was successfull\n",
      "importing rows 0 to 1000000...Done. 77.92512369155884 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 158.05686569213867 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 236.97258973121643 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 320.8134112358093 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 403.5784981250763 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 492.5768892765045 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 576.3061325550079 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 656.3751928806305 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 738.3340067863464 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 824.957765340805 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 909.865225315094 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 991.4730894565582 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 1074.111207485199 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 1157.1175556182861 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 1238.2929980754852 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 1321.1786687374115 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 1403.3949236869812 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 1486.544883966446 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 1571.4219992160797 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 1655.3204908370972 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 1737.173351764679 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 1820.2471418380737 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 1905.7811727523804 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 1990.4828374385834 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 2078.2971756458282 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 2163.7002828121185 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 2165.902898788452 total seconds elapsed\n",
      "ETL was completed\n"
     ]
    }
   ],
   "source": [
    "#Execute the ETL Process\n",
    "ETL_Movies_Process(wikipedia_filename,kaggle_meta_filname,kaggle_rating_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #def tranform_kaggle_ratings(kaggle_rating_filename):\n",
    "    ##Load ratings to data frame\n",
    "    #kaggle_ratings_df = pd.read_csv(f'{DATA_PATH}{kaggle_rating_filename}')\n",
    "    ##convert to datetime for timestamp\n",
    "    #kaggle_ratings_df['timestamp'] = pd.to_datetime(kaggle_ratings_df['timestamp'], unit='s', errors='coerce') \n",
    "    #return kaggle_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##performace this merge with SQL\n",
    "    ##Merge data on kaggle_id with left_on\n",
    "    ##create the pivot table with index is movieId\n",
    "    #kaggle_ratings_df = kaggle_ratings_df.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "    #                .rename({'userId':'count'}, axis=1) \\\n",
    "    #                .pivot(index='movieId',columns='rating', values='count')\n",
    "    ##Create Column names\n",
    "    #rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    ## merge two data frame on column and index\n",
    "    #movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    ## fill all raning is n/a to Zero\n",
    "    #movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
